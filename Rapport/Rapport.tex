\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{eurosym}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{float}
\usepackage{perpage}
\MakePerPage{footnote}

% Fallback for mathbb if not defined
\providecommand{\mathbb}[1]{\mathbf{#1}}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{longtable}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Rapport P1RV},
    pdfpagemode=FullScreen,
}
\renewcommand{\contentsname}{Table des matières}

\begin{document}
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge \textbf{Rapport de Projet P1RV}}
    
    \vspace{1.5cm}
    
    {\Large \textbf{Optimisation et Parallélisation du Pricing d'Options Américaines}}\\
    \vspace{0.5cm}
    {\large \textit{Méthode de Monte Carlo Longstaff-Schwartz sur CPU et GPU}}
    
    \vspace{3cm}
    
    \textbf{Auteurs :} \\
    \vspace{0.5cm}
    {\Large Florian BARBE} \\
    {\Large Narjisse EL MANSSOURI}
    
    \vspace{3cm}
    
    \vfill
    
    {\large École Centrale de Nantes} \\
    \vspace{0.2cm}
    {\large Année Universitaire 2025 -- 2026}
    
\end{titlepage}

\newpage
\tableofcontents
\newpage
% \listoffigures
\newpage

\section{Introduction}

Ce rapport synthétise les travaux d'inplémentation et d'optimisation de l'algorithme de Longstaff--Schwartz (LSMC) pour la valorisation d'options américaines. Ce projet explore l'impact des architectures de calcul sur la performance d'une méthode de Monte Carlo avec régression, intrinsèquement coûteuse en ressources.

L'objectif est d'évaluer les gains de performance offerts par le parallélisme sur trois architectures distinctes :
\begin{itemize}
    \item \textbf{CPU Séquentiel} : référence fonctionnelle.
    \item \textbf{CPU Multi-cœurs (OpenMP)} : exploitation du parallélisme à mémoire partagée.
    \item \textbf{GPU (CUDA)} : exploitation du parallélisme massif de données.
\end{itemize}

Ce document détaille la formulation mathématique, les choix d'implémentation algorithmique, et fournit une analyse critique des performances comparées (speedup, efficacité) et des limitations structurelles identifiées.

\section{Cadre de Modélisation}

\subsection{Dynamique du Sous-Jacent}

Le sous-jacent $(S_t)_{t\ge0}$ suit un Mouvement Brownien Géométrique (GBM) régi par l'EDS :
\[ \mathrm{d}S_t = r S_t\,\mathrm{d}t + \sigma S_t\,\mathrm{d}W_t^{\mathbb{Q}} \]
La solution explicite et sa discrétisation exacte sur une grille temporelle $\{t_0, \dots, t_N\}$ sont données par (voir Annexe~\ref{appendix:eds} pour la dérivation) :
\[ S_{t_{n+1}} = S_{t_n} \exp\left( (r - \frac{\sigma^2}{2})\Delta t + \sigma \sqrt{\Delta t} Z_n \right), \quad Z_n \sim \mathcal{N}(0,1) \]

\subsection{Problème d'Arrêt Optimal}

Contrairement à une option européenne, une option américaine offre à son détenteur la liberté d'exercer son droit à n'importe quelle date $t_n$ de la grille de discrétisation. Cette flexibilité introduit un \textbf{choix stratégique} à chaque pas de temps : exercer immédiatement pour encaisser le payoff intrinsèque, ou conserver l'option pour profiter d'une évolution favorable future.

On note $V_{t_n}$ la valeur de l'option à la date $t_n$ et $\Phi(S_{t_n})$ sa valeur d'exercice immédiat (pour un Put, $\Phi(S) = \max(K-S, 0)$).
La décision optimale repose sur la comparaison entre :
\begin{itemize}
    \item \textbf{La valeur d'exercice immédiat} : $\Phi(S_{t_n})$.
    \item \textbf{La valeur de continuation} : $C(t_n, S_{t_n})$, qui représente l'espérance des flux futurs actualisés si l'on choisit de ne pas exercer aujourd'hui.
\end{itemize}

Formellement, la valeur de continuation est définie par l'espérance conditionnelle sous la mesure risque-neutre $\mathbb{Q}$ :
\[
C(t_n, S_{t_n}) = \mathbb{E}^{\mathbb{Q}}\left[ e^{-r \Delta t} V_{t_{n+1}} \mid S_{t_n} \right]
\]

L'équation de programmation dynamique (Bellman) qui régit le prix de l'option est alors :
\[
V_{t_n} = \max\left( \Phi(S_{t_n}), \, C(t_n, S_{t_n}) \right)
\]

C'est cette quantité $C(t_n, S_{t_n})$, espérance conditionnelle inconnue, que l'algorithme LSMC cherche à estimer par régression.


\subsection{Algorithme de Longstaff--Schwartz (LSMC)}
L'algorithme LSMC \cite{LSM2001} résout ce problème par \emph{backward induction}, en approximant l'espérance conditionnelle de continuation par une régression sur une base de fonctions.

L'algorithme~\ref{alg:lsmc} formalise cette procédure.

\begin{algorithm}
\caption{Algorithme de Longstaff--Schwartz (LSMC)}
\label{alg:lsmc}
\begin{algorithmic}[1]
\STATE Simuler $N_{\text{paths}}$ trajectoires $(S_{t_n}^{(i)})_{n}$.
\STATE Initialiser $V_{t_N}^{(i)} = \Phi(S_{t_N}^{(i)})$.
\FOR{$n = N-1$ \textbf{down to} $1$}
    \STATE $Y^{(i)} = e^{-r \Delta t_n} V_{t_{n+1}}^{(i)}$.
    \STATE Régression $Y^{(i)} \approx \widehat{C}(t_n,S_{t_n}^{(i)})$.
    \FOR{chaque trajectoire $i$}
        \IF{$\Phi(S^{(i)}) \ge \widehat{C}(S^{(i)})$}
            \STATE $V^{(i)} \leftarrow \Phi(S^{(i)})$
        \ELSE
            \STATE $V^{(i)} \leftarrow Y^{(i)}$
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE $V_0 = \text{mean}(e^{-r t_1} V_{t_1}^{(i)})$.
\end{algorithmic}
\end{algorithm}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{images/Distributed-LSMC-algorithm-flow-chart.png}
    \caption{Flux de l'algorithme LSMC : Simulation parallèle vs Régression séquentielle.}
    \label{fig:lsmc_flow}
\end{figure}

\textbf{Synthèse :} La structure du LSMC est intrinsèquement hybride. Si la phase de simulation est un candidat idéal pour le parallélisme massif ("embarrassingly parallel"), la phase de backward induction impose une synchronisation globale à chaque pas de temps, limitant le potentiel d'accélération sur GPU (loi d'Amdahl).


\section{Travail réalisé : implémentation et méthodologie}

Cette section décrit les choix techniques et méthodologiques retenus pour
l’implémentation de l’algorithme de Longstaff--Schwartz, ainsi que les stratégies
de parallélisation mises en œuvre sur CPU et GPU. L’objectif est double :
assurer la validité numérique des résultats tout en évaluant l’impact des
architectures de calcul sur les performances.

\subsection{Environnement de développement}

L’ensemble du projet a été développé en \texttt{C++}, avec une attention
particulière portée aux performances et à la gestion mémoire. Les principaux
outils et technologies utilisés sont :
\begin{itemize}
    \item compilateur \texttt{g++} compatible \texttt{C++17} ;
    \item bibliothèque \texttt{OpenMP} pour la parallélisation CPU ;
    \item \texttt{CUDA C++} pour l’implémentation GPU ;
    \item générateurs pseudo-aléatoires indépendants par thread ;
    \item système de build basé sur \texttt{CMake}.
\end{itemize}

Les calculs ont été réalisés sur une machine équipée d’un processeur multi-cœurs
et d’un GPU NVIDIA compatible CUDA. Les expériences ont été conduites en
privilégiant la reproductibilité et la stabilité numérique.

\subsection{Implémentation CPU séquentielle}

Une première version séquentielle de l’algorithme LSMC a été implémentée afin de
servir de référence fonctionnelle et de point de comparaison en termes de
performance.

Cette version suit strictement les étapes théoriques :
\begin{itemize}
    \item simulation des trajectoires du sous-jacent selon un mouvement brownien
    géométrique ;
    \item calcul des payoffs à chaque date ;
    \item application de la backward induction ;
    \item estimation de la valeur de continuation par régression polynomiale ;
    \item calcul de la moyenne finale des cashflows actualisés.
\end{itemize}

L’implémentation séquentielle permet de valider la cohérence des résultats
numériques et de mesurer le coût de calcul intrinsèque de l’algorithme avant
toute parallélisation.

\subsection{Parallélisation CPU avec OpenMP}

La version parallèle CPU repose sur l’observation que la majorité des calculs
dans l’algorithme LSMC sont indépendants par trajectoire. Cette propriété est
exploitée à l’aide d’OpenMP.

Les sections parallélisées sont notamment :
\begin{itemize}
    \item la génération des trajectoires du sous-jacent ;
    \item le calcul des payoffs ;
    \item l’accumulation des termes des équations normales pour la régression ;
    \item la mise à jour des cashflows lors de la backward induction ;
    \item le calcul de la moyenne finale.
\end{itemize}

Les réductions OpenMP sont utilisées pour agréger efficacement les contributions
des différentes trajectoires, tout en garantissant l’absence de conditions de
course\footnote{Une \emph{race condition} (condition de course) survient lorsque le résultat d'un programme dépend de l'ordre d'exécution imprévisible de threads accédant simultanément à des données partagées en écriture.}. Le schéma \texttt{schedule(static)} est privilégié afin d’assurer une
répartition homogène de la charge de travail et un bon comportement mémoire.

Cette parallélisation permet d’exploiter efficacement les cœurs du processeur,
mais reste limitée par la bande passante mémoire et la nature statistique de
l’algorithme.

\subsection{Accélération GPU avec CUDA}

Une version accélérée sur GPU a été développée afin d’exploiter le parallélisme
massif offert par les architectures CUDA, une approche explorée dans des travaux similaires \cite{Oger, Croain}. Le GPU est particulièrement adapté à
la simulation Monte Carlo, chaque trajectoire pouvant être associée à un thread
indépendant.

Les principales étapes déportées sur le GPU sont :
\begin{itemize}
    \item la simulation des trajectoires du mouvement brownien géométrique ;
    \item le calcul des payoffs ;
    \item certaines phases de réduction nécessaires à la régression.
\end{itemize}

La backward induction impose cependant une dépendance temporelle forte entre les
dates successives, ce qui limite la parallélisation complète de l’algorithme.
L’approche retenue consiste donc à paralléliser intensivement les calculs
spatiaux (trajectoires) tout en conservant une synchronisation globale entre les
dates.

Une attention particulière est portée à l’organisation mémoire des données afin
de garantir des accès coalescents\footnote{L'accès coalescent désigne le regroupement par le matériel de plusieurs requêtes mémoire provenant de threads voisins en une seule transaction physique, optimisant ainsi l'utilisation de la bande passante.} et de limiter les transferts entre l’hôte
(CPU) et le périphérique (GPU).

Cette implémentation permet d’évaluer concrètement les gains de performance
apportés par le GPU et de mettre en évidence les limites structurelles du LSMC
dans un contexte massivement parallèle.

\subsection{Méthodes de Différences Finies (FDM) pour comparaison}

Afin de valider nos résultats Monte Carlo et de disposer d'un point de comparaison déterministe performant, nous avons également implémenté des solveurs basés sur les différences finies (FDM) pour l'équation de Black-Scholes-Merton (PDE) \cite{Hull}. Bien que ces méthodes soient limitées en dimension (difficiles à étendre au-delà de 2 ou 3 sous-jacents), elles sont extrêmement efficaces pour les options vanilles et américaines sur un seul sous-jacent.

Trois schémas numériques ont été implémentés dans le fichier \texttt{fdm.cpp} :
\begin{itemize}
    \item Le schéma de \textbf{Runge-Kutta 4 (RK4)} a été choisi comme méthode de référence ("baseline") pour sa haute précision ($O(\Delta t^4)$). Bien que plus coûteux en calcul que les méthodes d'Euler, il fournit une valeur quasi-exacte pour valider les résultats Monte Carlo.
    \item \textbf{Euler Implicite} : Schéma inconditionnellement stable, nécessitant la résolution d'un système linéaire tridiagonal à chaque pas de temps (algorithme de Thomas\footnote{L'algorithme de Thomas, également appelé TDMA (TriDiagonal Matrix Algorithm), est une forme simplifiée de l'élimination de Gauss optimisée pour les systèmes tridiagonaux. Sa complexité est (n)$ contre (n^3)$ pour l'élimination de Gauss générale.}).
    \item \textbf{Euler Explicite} : Schéma simple et rapide, mais conditionnellement stable (nécessite un pas de temps suffisamment petit pour éviter les instabilités numériques).
\end{itemize}

Ces méthodes nous ont servi de "vérité terrain" pour vérifier la convergence de nos prix LSMC.

\subsection{Structure du code et organisation des fichiers}

Le projet est organisé de manière modulaire pour séparer les responsabilités (modélisation, calcul, utilitaires). Voici une description de l'arborescence :

\begin{itemize}
    \item \texttt{src/} (dossier racine \texttt{P1RV\_CUDA/}) :
    \begin{itemize}
        \item \texttt{lsmc.cu} / \texttt{lsmc.cpp} : Cœur de l'algorithme Longstaff-Schwartz. La version \texttt{.cu} contient les kernels CUDA pour le GPU.
        \item \texttt{gbm.cu} : Simulation du Mouvement Brownien Géométrique.
        \item \texttt{fdm.cpp} : Implémentation des méthodes de différences finies (solvers PDE).
        \item \texttt{main.cu} : Point d'entrée, gestion des arguments et lancement des benchmarks.
    \end{itemize}
    \item \texttt{Utils/} :
    \begin{itemize}
        \item \texttt{csv\_writer.hpp} : Utilitaires pour l'export des résultats.
        \item Scripts Python : Pour l'interface utilisateur et la visualisation.
    \end{itemize}
\end{itemize}



\section{Résultats et analyse des performances}

\subsection{Configuration matérielle}

Tous les benchmarks présentés dans cette section ont été réalisés sur une machine équipée d'un processeur \textbf{Intel® Core™ i7-13620H} (13ème génération) et d'une carte graphique \textbf{NVIDIA GeForce RTX 4060}. Cette dernière, basée sur l'architecture Ada Lovelace, dispose de \textbf{3072 cœurs CUDA} et de \textbf{8 Go de mémoire GDDR6}, offrant une puissance de calcul théorique adaptée aux simulations massivement parallèles.

Cette section présente les résultats obtenus lors de l’exécution de l’algorithme
de Longstaff--Schwartz selon les différentes architectures étudiées : CPU
séquentiel, CPU parallélisé avec OpenMP et GPU via CUDA.  
L’analyse porte à la fois sur la validité numérique des résultats et sur les
performances de calcul observées.

\subsection{Première approche : comparaison des méthodes de parallélisme}

Les performances ont été évaluées sur un ensemble de simulations Monte Carlo
pour une option de type put américain. Nous avons comparé les temps d'exécution et la précision des prix obtenus par nos trois implémentations (CPU Séquentiel, OpenMP, GPU CUDA) ainsi que par les méthodes de différences finies.

Les résultats ci-dessous ont été obtenus sur une machine équipée d'un GPU NVIDIA.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|c|r|r|}
\hline
\textbf{Mode} & \textbf{Pas ($N$)} & \textbf{Trajectoires ($M$)} & \textbf{Prix (\euro)} & \textbf{Temps (ms)} & \textbf{Écart / FDM} \\
\hline
FDM Implicite & 1000 & - & 6.067 & 0.67 & Ref \\
FDM Explicite & 1000 & - & 6.079 & 60.18 & +0.012 \\
FDM RK4 & 1000 & - & 6.079 & 231.88 & +0.012 \\
\hline
CPU Séquentiel & 50 & 100,000 & 6.057 & 559.91 & -0.010 \\
OpenMP & 50 & 100,000 & 6.057 & 540.23 & -0.010 \\
\textbf{GPU CUDA} & 50 & 100,000 & 6.070 & \textbf{41.96} & +0.003 \\
\hline
CPU Séquentiel & 50 & 1,000,000 & 6.059 & 6914.19 & -0.008 \\
OpenMP & 50 & 1,000,000 & 6.059 & 6562.99 & -0.008 \\
\textbf{GPU CUDA} & 50 & 1,000,000 & 6.047 & \textbf{455.63} & -0.020 \\
\hline
CPU Séquentiel & 50 & 5,000,000 & 6.057 & 35352.25 & -0.010 \\
\hline
\end{tabular}
\caption{Comparaison des temps de calcul et précision pour un Put Américain ($S_0=100, K=100, r=0.05, \sigma=0.2, T=1$). (Matériel : i7-13620H + RTX 4060)}
\label{tab:results}
\end{table}

\paragraph{Analyse des résultats :}
\begin{itemize}
    \item \textbf{Précision} : Tous les modes LSMC convergent vers un prix très proche de la référence FDM (~6.067). Les écarts observés sont de l'ordre du centime, ce qui est acceptable pour une méthode de Monte Carlo avec ces paramètres.
    \item \textbf{Performance GPU} : Le GPU démontre une accélération spectaculaire. Pour 1 million de trajectoires, le calcul prend environ \textbf{455 ms} sur GPU contre près de \textbf{7 secondes} (6914 ms) sur CPU séquentiel, soit un facteur d'accélération (speedup) d'environ $\times 15$.
    \item \textbf{Comparaison FDM} : Les méthodes de différences finies (surtout l'implicite) sont extrêmement rapides (< 1ms) pour ce problème 1D. Cela confirme que pour des options simples, les PDE restent supérieures. Cependant, l'intérêt du LSMC (et donc de notre implémentation GPU) réside dans sa capacité à traiter des problèmes de plus haute dimension où les méthodes de grille échouent.
\end{itemize}

\subsection{Pour aller plus loin : analyse de l'impact de la base de régression}

Les benchmarks réalisés ("boosted", voir tableau~\ref{tab:boosted}) révèlent que l'augmentation du degré de la base est bénéfique. La base Cubique permet d'atteindre un prix de $\approx 6.06$, nettement plus proche de la référence théorique ($\approx 6.08$) que les bases quadratiques/monomiales ($\approx 5.58$ dans cette configuration CPU non-optimisée). 

Cette amélioration de précision justifie le léger surcoût calculatoire lié à la résolution de systèmes linéaires $4 \times 4$, désormais gérée par notre solveur de Gauss générique sur GPU.

\begin{table}[H]
\centering
\scriptsize
\begin{tabular}{|l|l|c|r|r|r|}
\hline
\textbf{Base} & \textbf{Architecture} & \textbf{Trajectoires} & \textbf{Prix (\euro)} & \textbf{Temps (ms)} & \textbf{Throughput (ops/s)} \\
\hline
\multicolumn{6}{|c|}{\textbf{N = 100,000}} \\
\hline
Monomiale & CPU Séquentiel & 100k & 5.586 & 261.96 & 19.1 M \\
Monomiale & CPU OpenMP & 100k & 5.586 & 274.97 & 18.2 M \\
Monomiale & GPU & 100k & 6.070 & 45.46 & 109.9 M \\
\hline
Hermite & CPU Séquentiel & 100k & 5.586 & 265.94 & 18.8 M \\
Hermite & CPU OpenMP & 100k & 5.586 & 266.81 & 18.7 M \\
Hermite & GPU & 100k & 6.070 & 43.40 & 115.2 M \\
\hline
Laguerre & CPU Séquentiel & 100k & 5.586 & 268.81 & 18.6 M \\
Laguerre & CPU OpenMP & 100k & 5.586 & 265.83 & 18.8 M \\
Laguerre & GPU & 100k & 6.070 & 35.44 & 141.1 M \\
\hline
Chebyshev & CPU Séquentiel & 100k & 5.586 & 263.63 & 18.9 M \\
Chebyshev & CPU OpenMP & 100k & 5.586 & 265.64 & 18.8 M \\
Chebyshev & GPU & 100k & 6.070 & 41.66 & 120.0 M \\
\hline
Cubique & CPU Séquentiel & 100k & 5.586 & 266.56 & 18.7 M \\
Cubique & CPU OpenMP & 100k & 5.586 & 265.21 & 18.8 M \\
Cubique & GPU & 100k & 6.086 & 38.66 & 129.3 M \\
\hline
\multicolumn{6}{|c|}{\textbf{N = 1,000,000}} \\
\hline
Monomiale & CPU Séquentiel & 1M & 5.565 & 2653.88 & 18.8 M \\
Monomiale & CPU OpenMP & 1M & 5.565 & 2688.26 & 18.6 M \\
Monomiale & GPU & 1M & 6.047 & 334.53 & 149.5 M \\
\hline
Hermite & CPU Séquentiel & 1M & 5.565 & 2672.32 & 18.7 M \\
Hermite & CPU OpenMP & 1M & 5.565 & 2704.67 & 18.5 M \\
Hermite & GPU & 1M & 6.047 & 594.25 & 84.1 M \\
\hline
Laguerre & CPU Séquentiel & 1M & 5.565 & 2710.66 & 18.4 M \\
Laguerre & CPU OpenMP & 1M & 5.565 & 2841.14 & 17.6 M \\
Laguerre & GPU & 1M & 6.047 & 649.80 & 76.9 M \\
\hline
Chebyshev & CPU Séquentiel & 1M & 5.565 & 2633.49 & 18.9 M \\
Chebyshev & CPU OpenMP & 1M & 5.565 & 2638.32 & 18.9 M \\
Chebyshev & GPU & 1M & 6.047 & 626.32 & 79.8 M \\
\hline
Cubique & CPU Séquentiel & 1M & 5.565 & 2677.14 & 18.7 M \\
Cubique & CPU OpenMP & 1M & 5.565 & 2628.52 & 19.0 M \\
Cubique & GPU & 1M & 6.063 & 687.69 & 72.7 M \\
\hline
\end{tabular}
\caption{Benchmarks "Boosted" complets : Impact du choix de la base et de l'architecture. (Matériel : i7-13620H + RTX 4060)}
\label{tab:boosted}
\end{table}

Les temps mesurés confirment l'efficacité de l'approche massivement parallèle pour la phase de simulation. Le goulot d'étranglement restant sur GPU est la régression séquentielle à chaque pas de temps.

\subsection{Validation numérique}

Avant toute analyse de performance, la cohérence numérique des différentes
implémentations a été vérifiée.  
Les prix obtenus avec :
\begin{itemize}
    \item l’implémentation CPU séquentielle,
    \item l’implémentation CPU OpenMP,
    \item l’implémentation GPU CUDA,
\end{itemize}
sont compatibles entre eux à l’intérieur des intervalles d’erreur statistique
attendus pour une méthode de Monte Carlo.

Lorsque le nombre de trajectoires augmente, la variance de l’estimateur décroît
conformément au taux théorique $\mathcal{O}(1/\sqrt{N_{\text{paths}}})$, ce qui
confirme la bonne implémentation de l’algorithme LSMC sur les trois architectures.

\subsection{Performances CPU séquentiel}

L’implémentation séquentielle sert de référence de performance.  
Le temps d’exécution croît linéairement avec le nombre de trajectoires simulées,
ce qui est cohérent avec la complexité algorithmique du LSMC :
\[
\mathcal{O}(N_{\text{paths}} \times N_{\text{steps}}).
\]

Cette version met en évidence le caractère fortement coûteux des méthodes de
Monte Carlo lorsque la précision recherchée impose un grand nombre de trajectoires.
Elle justifie pleinement le recours à des techniques de parallélisation.

Les figures ci-dessous illustrent la validation empirique de la complexité linéaire de l'implémentation séquentielle.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_paths.png}
        \caption{Temps vs $N_{paths}$ (CPU Séquentiel)}
        \label{fig:linearity_paths}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_steps.png}
        \caption{Temps vs $N_{steps}$ (CPU Séquentiel)}
        \label{fig:linearity_steps}
    \end{minipage}
\end{figure}

L'alignement quasi-parfait des points mesurés avec la régression linéaire confirme que l'implémentation respecte la complexité théorique sans overhead significatif cachés.

\subsection{Accélération par parallélisation CPU avec OpenMP}

La version OpenMP exploite le parallélisme multi-cœurs du processeur.  
Les gains observés sont significatifs pour les phases suivantes :
\begin{itemize}
    \item simulation des trajectoires du sous-jacent ;
    \item calcul des payoffs ;
    \item accumulation des équations normales pour la régression ;
    \item mise à jour des cashflows.
\end{itemize}

Les résultats de benchmark avec 4 threads OpenMP confirment une amélioration des temps de calcul, tout en conservant une complexité linéaire globale (voir figures ci-dessous).

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_paths_omp.png}
        \caption{Temps vs $N_{paths}$ (OpenMP 4 threads)}
        \label{fig:linearity_paths_omp}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_steps_omp.png}
        \caption{Temps vs $N_{steps}$ (OpenMP 4 threads)}
        \label{fig:linearity_steps_omp}
    \end{minipage}
\end{figure}

Le facteur d’accélération obtenu est inférieur au nombre de cœurs disponibles,
ce qui s’explique par :
\begin{itemize}
    \item la bande passante mémoire limitée ;
    \item les accès concurrents aux structures de données partagées ;
    \item la présence de phases séquentielles incompressibles (notamment la
    progression temporelle de la backward induction).
\end{itemize}

Néanmoins, OpenMP permet une réduction notable du temps de calcul, tout en conservant une implémentation relatively simple et portable.

\textbf{Synthèse :} Le parallélisme OpenMP est efficace pour les phases \emph{compute-bound} (simulations) mais sature rapidement la bande passante mémoire lors des opérations vectorielles massives, plafonnant l'accélération bien en deçà du nombre de cœurs physiques.

\subsection{Accélération GPU avec CUDA}

L’implémentation GPU avec CUDA a mis en évidence \cite{Benguigui} un contraste marqué entre les
différentes phases de l’algorithme. La simulation des trajectoires et le calcul
des payoffs bénéficient pleinement du parallélisme massif, avec des accélérations
importantes lorsque le nombre de trajectoires augmente.

Les résultats expérimentaux (voir Figures ci-dessous) montrent un gain de performance croissant avec la
taille du problème, atteignant des facteurs d’accélération supérieurs à $30$
pour de grands nombres de trajectoires.

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_paths_gpu.png}
        \caption{Temps vs $N_{paths}$ (GPU CUDA)}
        \label{fig:linearity_paths_gpu}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/linearity_steps_gpu.png}
        \caption{Temps vs $N_{steps}$ (GPU CUDA)}
        \label{fig:linearity_steps_gpu}
    \end{minipage}
\end{figure}

En revanche, la backward induction impose une dépendance temporelle forte entre les dates successives.
Cette contrainte limite la parallélisation complète de l'algorithme et réduit le gain théorique maximal.
Les performances dépendent notamment :
\begin{itemize}
    \item du nombre de trajectoires simulées ;
    \item de l'occupation effective du GPU ;
    \item de l'efficacité des réductions parallèles ;
    \item de la limitation des transferts mémoire CPU-GPU.
\end{itemize}

\textbf{Synthèse :} Le GPU excelle sur le parallélisme de données (trajectoires), offrant des speedups d'un ordre
de grandeur, mais l'accélération globale reste structurellement bornée par la nature séquentielle temporelle
de l'algorithme (loi d'Amdahl appliquée à la backward induction).

Par ailleurs, une légère différence entre les prix CPU et GPU a été observée.
Elle s’explique par la nature statistique de la méthode de Monte Carlo, les
différences de générateurs pseudo-aléatoires et les effets d’arrondis en
arithmétique flottante. Ces écarts restent toutefois compatibles avec la variance
attendue de l’estimateur.

\subsection{Comparaison globale des architectures}

De manière synthétique :
\begin{itemize}
    \item le CPU séquentiel fournit une référence simple mais peu performante ;
    \item OpenMP permet un gain modéré, limité par la bande passante mémoire ;
    \item CUDA offre les meilleures performances pour les grandes tailles de
    problèmes, en particulier lorsque le nombre de trajectoires est très élevé.
\end{itemize}

Le GPU s’avère donc particulièrement adapté aux méthodes de Monte Carlo à grande
échelle, tandis que le CPU reste pertinent pour des tailles de problèmes plus
modérées ou lorsque la simplicité d’implémentation est prioritaire.

\subsection{Discussion sur les limites du parallélisme}

Les résultats confirment que l’algorithme LSMC est naturellement bien adapté au parallélisme spatial, mais intrinsèquement limité par sa structure séquentielle dans le temps. En effet, l'algorithme repose sur une \textbf{induction arrière} (backward induction) : le calcul des valeurs à l'étape $t$ dépend mathématiquement des résultats de l'étape $t+1$ (nécessaires pour construire la variable cible de la régression).

Il est donc impossible de paralléliser le traitement des différents pas de temps pour une même option. Le GPU doit impérativement attendre la fin du calcul de l'étape $t+1$ avant de commencer l'étape $t$, ce qui crée une barrière de synchronisation inévitable. L'accélération maximale est ainsi plafonnée par cette contrainte séquentielle, même avec un nombre infini de cœurs. La seule manière d'accroître davantage le parallélisme serait de traiter simultanément plusieurs options distinctes (batching).

Ces observations expliquent pourquoi les gains GPU, bien que très importants, ne peuvent pas être parfaitement linéaires avec la puissance de calcul.

Ces résultats mettent en évidence l’intérêt d’architectures hybrides CPU--GPU, ainsi que l’importance de choix d’implémentation fins (organisation mémoire, réductions efficaces, limitation des synchronisations) pour exploiter pleinement les capacités du matériel moderne.

\textbf{Synthèse Globale :} Si le GPU déverrouille la scalabilité spatiale du stock de trajectoires, la dimension temporelle reste séquentielle. L'optimum de performance réside dans un équilibre : assez de trajectoires pour saturer le GPU, mais pas au-delà des besoins de précision, car le coût de régression devient alors prépondérant.

\section{Analyse Critique et Limitations}

La réalisation de ce projet a mis en évidence plusieurs défis techniques et méthodologiques, dont l'analyse permet de mieux cerner les contraintes du calcul haute performance en finance.

\subsection{Organisation du projet et évolution des dépôts}

Le développement du projet s'est articulé autour de \textbf{deux dépôts Git distincts}, reflétant une évolution technique majeure :

\begin{enumerate}
    \item \textbf{lsmc-openmp} (Octobre 2025) : Le premier dépôt a été consacré au développement de la logique métier de l'algorithme LSMC en C++ avec parallélisation OpenMP. Le système de build reposait sur des solutions \texttt{Visual Studio} (.sln, .vcxproj). Ce dépôt inclut également une interface graphique en Python (Streamlit/Matplotlib) pour la visualisation des trajectoires.
    
    \item \textbf{CUDA-Implementation} (Janvier 2026) : Face aux difficultés d'intégration CUDA dans le premier dépôt, un second dépôt a été créé avec une architecture repensée autour de \textbf{CMake}. C'est dans ce dépôt que les fonctionnalités avancées (5 bases de régression, solveur générique, kernels optimisés) ont été développées et validées.
\end{enumerate}

Cette organisation permet de conserver une \textbf{version CPU de référence} (premier dépôt) tout en disposant d'une \textbf{version HPC/GPU complète} (second dépôt).

\subsection{Échec de l'intégration CUDA dans Visual Studio}

L'historique des commits du premier dépôt témoigne des tentatives infructueuses d'intégration CUDA :
\begin{itemize}
    \item \textit{"Début intégration CUDA"} : ajout initial des fichiers .cu et configuration NVCC.
    \item \textit{"On continue de debug CUDA car rien ne marche"} : erreurs de compilation persistantes.
    \item \textit{"Suppression complète de tout le code lié à CUDA/GPU"} : abandon de l'approche.
\end{itemize}

\textbf{Cause identifiée :} Visual Studio peinait à gérer correctement la cohabitation entre le compilateur C++ standard (MSVC) et le compilateur CUDA (\texttt{nvcc}). Les conflits de flags de compilation, les incompatibilités d'architectures cibles et la gestion des dépendances rendaient la configuration instable.

\textbf{Solution adoptée :} La migration vers \textbf{CMake} a résolu ces problèmes. CMake intègre nativement le support CUDA via \texttt{enable\_language(CUDA)} et permet une séparation propre entre le code hôte (.cpp) et le code device (.cu). Cette architecture a permis de finaliser l'implémentation GPU avec succès.

\subsection{Complexité de l’exercice anticipé}

La difficulté théorique majeure provient de la possibilité d’un exercice anticipé
pour les options américaines. Cette caractéristique transforme le problème de
valorisation en un problème d’arrêt optimal, nécessitant une estimation fiable
de la valeur de continuation à chaque date. Une mauvaise compréhension de cette
quantité conduit rapidement à des erreurs de logique dans la backward induction.
Un soin particulier a donc été apporté à la séparation claire entre valeur
d’exercice immédiat et valeur de continuation estimée.

\subsection{Choix, stabilité et extensions de la régression}

L’estimation de la valeur de continuation par régression constitue un point
sensible de l’algorithme. Le choix des fonctions de base représente un compromis
entre précision et stabilité numérique. Des bases trop simples introduisent un
biais, tandis que des bases trop riches peuvent engendrer des instabilités ou un
surcoût de calcul.

\paragraph{Étude initiale : Bases quadratiques.}
Dans un premier temps, une étude comparative a été menée entre la base canonique $(1, S, S^2)$ et la base de \textbf{polynômes d'Hermite de degré 2} $(1, S, S^2-1)$. Bien que les benchmarks montrent peu de différence en termes de précision pour ce problème spécifique, la base d'Hermite a été retenue par défaut pour ses meilleures propriétés théoriques de conditionnement (matrice $A^T A$).

\paragraph{Extensions et bases avancées.}
Afin d'affiner la capture de la valeur de continuation, nous avons étendu l'implémentation à trois autres familles de bases, souvent citées dans la littérature spécialisée \cite{Hull} :
\begin{enumerate}
    \item \textbf{Laguerre} : Polynômes orthogonaux sur $[0, \infty[$, historiquement suggérés par Longstaff et Schwartz \cite{LSM2001} pour leur adaptation naturelle aux prix d'actifs positifs (pondération par exponentielle décroissante).
    \item \textbf{Tchebychev} : Polynômes minimisant l'erreur d'interpolation sur $[-1, 1]$. Cette base a nécessité l'implémentation d'un kernel de réduction Min-Max sur GPU pour normaliser les prix à chaque pas de temps.
    \item \textbf{Cubique} : Base monômiale enrichie au degré 3 $(1, S, S^2, S^3)$.
\end{enumerate}


La figure~\ref{fig:basis_comparison} illustre l'impact du choix de la base de régression sur la précision de l'estimation.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{images/precision_convergence_full.png}
    \caption{Comparaison de la convergence de l'erreur relative selon le nombre de trajectoires ($N=10^4, 10^5$) et l'architecture.}
    \label{fig:degree_convergence_full}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\linewidth]{images/Capture d'écran 2026-01-10 160639.png}
    \caption{Évolution théorique attendue de la précision (Source: \cite{ArealParallelMethods}). Voici ce que l'on devrait observer idéalement.}
    \label{fig:basis_comparison}
\end{figure}

\paragraph{Analyse de la convergence (Figure~\ref{fig:degree_convergence_full}).}
Les résultats présentés en figure~\ref{fig:degree_convergence_full} montrent l'évolution de l'erreur relative par rapport au prix RK4 de référence. Cette dynamique peut être mise en perspective avec la Figure~\ref{fig:basis_comparison} (issue de \cite{ArealParallelMethods}) qui illustre les attentes théoriques de convergence.
Contrairement à l'intuition théorique qui suggère une amélioration monotone, les courbes observées ne présentent pas une décroissance régulière.
Plusieurs facteurs peuvent expliquer ce comportement mitigé et "non concluant" sur ce cas test spécifique :
\begin{itemize}
    \item \textbf{Phénomène de Runge :} L'utilisation de bases polynomiales de degré élevé sur des points non uniformément répartis (les trajectoires stochastiques) peut induire de fortes oscillations aux bords du domaine (valeurs extrêmes de $S_t$), dégradant la qualité globale de la régression \cite{Glasserman}.
    \item \textbf{Conditionnement de la matrice :} Pour des degrés élevés ($d > 5$), la matrice de Gram $A^T A$ devient mal conditionnée, en particulier pour la base Monomiale, ce qui amplifie les erreurs numériques lors de la résolution du système linéaire. Bien que les bases orthogonales (Laguerre, Hermite) soient censées atténuer ce problème, le bruit de Monte Carlo semble ici dominer les gains théoriques.
    \item \textbf{Compromis Biais-Variance :} Augmenter le degré réduit le biais de modélisation (capacité à fitter la "vraie" fonction de continuation) mais augmente la variance de l'estimateur, car le modèle capture le bruit statistique des trajectoires (sur-apprentissage).
\end{itemize}
Ces observations soulignent la difficulté pratique d'obtenir une convergence "parfaite" avec des méthodes de régression globale sur des degrés élevés sans un nombre de trajectoires extrêmement grand ($N \gg 10^5$).



\subsection{Compromis précision--performance}

Enfin, ce projet a mis en évidence le compromis fondamental entre précision
numérique et temps de calcul. L’augmentation du nombre de trajectoires améliore
la convergence statistique, mais accroît fortement le coût de calcul, en
particulier lors des phases de régression et de backward induction. Ce compromis
a guidé le choix des paramètres expérimentaux et l’analyse comparative des
architectures CPU et GPU.

\section{Perspectives et améliorations possibles}

Le travail réalisé dans le cadre de ce projet a permis de mettre en œuvre une version fonctionnelle et performante de l’algorithme de Longstaff--Schwartz sur différentes architectures de calcul. Plusieurs pistes d’amélioration et d’extension peuvent toutefois être envisagées, tant sur le plan algorithmique que sur le plan des performances et des modèles financiers considérés.

\subsection{Améliorations algorithmiques}

Une première piste d’amélioration concerne le choix des fonctions de base utilisées pour l’approximation de la valeur de continuation. Dans ce projet, une base polynomiale simple de degré deux a été retenue pour des raisons de simplicité et de stabilité numérique. Il serait possible d’explorer :
\begin{itemize}
    \item des bases polynomiales de degré plus élevé ;
    \item des polynômes orthogonaux (Laguerre, Hermite), souvent utilisés dans la littérature ;
    \item des bases adaptatives dépendant de la distribution du sous-jacent, comme suggéré dans des publications récentes \cite{Risks}.
\end{itemize}

Ces choix peuvent améliorer la précision de l’estimation de la valeur de continuation, au prix d’un coût de calcul plus élevé et d’un risque accru de sur-apprentissage.

Par ailleurs, l’utilisation de techniques de réduction de variance (antithetic variates, control variates, stratified sampling) pourrait significativement améliorer la convergence statistique de la méthode de Monte Carlo, en réduisant le nombre de trajectoires nécessaires pour atteindre une précision donnée.

\subsection{Extensions du modèle financier}

Le cadre retenu dans ce projet repose sur un mouvement brownien géométrique, modèle de référence mais relativement simplificateur. Plusieurs extensions naturelles peuvent être envisagées :
\begin{itemize}
    \item introduction d’un taux de dividende continu ;
    \item prise en compte d’une volatilité locale ou stochastique (modèles de Heston, SABR) ;
    \item extension à des options multi-actifs ou dépendant de plusieurs sous-jacents ;
    \item valorisation de produits exotiques présentant des payoffs path-dépendants.
\end{itemize}

L’algorithme de Longstaff--Schwartz conserve une grande flexibilité face à ces extensions, ce qui constitue l’un de ses principaux avantages par rapport aux méthodes analytiques.

\subsection{Optimisations CPU avancées}

Sur CPU, plusieurs optimisations supplémentaires pourraient être envisagées :
\begin{itemize}
    \item vectorisation explicite via les instructions SIMD (AVX2, AVX-512) ;
    \item meilleure gestion de la localité mémoire pour réduire la pression sur la bande passante RAM ;
    \item utilisation de bibliothèques numériques optimisées pour les régressions linéaires.
\end{itemize}

Ces optimisations permettraient d’exploiter plus finement l’architecture matérielle, en particulier pour des tailles de problème intermédiaires où le GPU n’est pas toujours optimal.

\subsection{Optimisation et généralisation de l’implémentation GPU}

Du côté GPU, plusieurs améliorations sont envisageables :
\begin{itemize}
    \item implémentation complète de la régression par moindres carrés directement sur GPU, sans synchronisation CPU ;
    \item utilisation de bibliothèques CUDA spécialisées (\texttt{cuBLAS}, \texttt{CUB}, \texttt{Thrust}) pour les réductions et opérations linéaires ;
    \item exploration de stratégies multi-GPU pour des simulations de très grande dimension.
\end{itemize}

Une telle approche permettrait de réduire encore les coûts de communication et d’atteindre des gains de performance plus proches du potentiel théorique du GPU.

\subsection{Perspectives méthodologiques}

Enfin, ce projet ouvre la voie à des comparaisons plus larges entre différentes approches numériques pour le pricing d’options américaines. Il serait intéressant de confronter les performances du LSMC à :
\begin{itemize}
    \item .
\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\linewidth]{images/Capture d'écran 2026-01-10 161107.png}
    \caption{Tableau comparatif des méthodes de pricing : arbres, itératives, Monte Carlo et transformée de Fourier \cite{ArealParallelMethods}.}
    \label{fig:methods_comparison}
\end{figure}
On pourrait ainsi comparer ;
    \item des approches par équations aux dérivées partielles résolues numériquement ;
    \item des méthodes récentes basées sur l’apprentissage automatique (réseaux de neurones pour l’approximation de la valeur de continuation).
\end{itemize}

Ces perspectives permettraient d’évaluer plus finement les compromis entre précision, coût de calcul et flexibilité des différentes méthodes, dans un contexte de finance quantitative moderne.

\section{Organisation du travail}

Le projet P1RV a été mené sur l’ensemble du premier semestre selon une
organisation progressive, combinant approfondissement théorique,
développement logiciel itératif et analyse des performances.  
Le travail s’est appuyé sur une démarche incrémentale, avec des phases
successives de conception, d’implémentation, de refactorisation et
d’optimisation, comme en témoigne l’historique détaillé du dépôt Git.

\subsection{Découpage du projet}

Le développement du projet a été structuré autour des grandes étapes suivantes :
\begin{itemize}
    \item étude du cadre théorique : options américaines, Monte Carlo,
    backward induction et algorithme de Longstaff--Schwartz ;
    \item implémentation d’une version CPU séquentielle servant de référence ;
    \item restructuration complète du code afin de séparer clairement
    simulation, régression et logique LSMC ;
    \item parallélisation CPU avec OpenMP ;
    \item ajout progressif d’un backend GPU CUDA expérimental ;
    \item mise en place d’outils d’export des résultats (CSV) et de visualisation ;
    \item campagnes de tests, mesures de performances et nettoyage final du code ;
    \item rédaction et structuration du rapport.
\end{itemize}

Ce découpage a permis de valider progressivement chaque brique fonctionnelle
avant d’aborder les aspects avancés de parallélisation et d’optimisation.

\subsection{Organisation du développement et itérations}

Le développement s’est appuyé sur un processus itératif, visible dans
l’historique du dépôt Git :
\begin{itemize}
    \item création initiale du projet et mise en place de la structure
    \texttt{src/include} ;
    \item premières implémentations du GBM, de la régression OLS et du LSMC ;
    \item phases de refonte complètes du code afin d’améliorer la lisibilité,
    la modularité et les performances ;
    \item intégration progressive d’OpenMP sur les boucles critiques ;
    \item ajout d’une interface de visualisation et d’export des résultats
    (scripts Python, Streamlit) ;
    \item implémentation d’un backend CUDA complet incluant la simulation GBM,
    le calcul des payoffs, la backward induction et les réductions nécessaires
    à la régression ;
    \item nettoyage approfondi du dépôt après des problèmes liés à des fichiers
    CSV volumineux et à l’historique Git.
\end{itemize}

Cette approche incrémentale a permis de maintenir un code fonctionnel à chaque
étape tout en introduisant progressivement des optimisations plus complexes.

\subsection{Répartition des tâches et binôme}

Afin d'optimiser notre efficacité, nous nous sommes réparti les tâches selon nos affinités et compétences techniques :

\begin{itemize}
    \item \textbf{Florian} : Responsable du "cœur de calcul" (\emph{Core Engine}).
    \begin{itemize}
        \item Modélisation mathématique et implémentation C++.
        \item Développement de l'algorithme LSMC et des solveurs FDM.
        \item Parallélisation CPU avec OpenMP.
        \item Développement complet du kernel CUDA et de l'implémentation GPU.
        \item Benchmarking et optimisation des performances bas niveau.
    \end{itemize}
    
    \item \textbf{Narjisse} : Responsable de l'expérience utilisateur et de l'interface (\emph{Frontend}).
    \begin{itemize}
        \item Conception de l'interface graphique (GUI) pour rendre l'outil accessible.
        \item Développement d'une application interactive (via Streamlit/Python) permettant de modifier les paramètres ($S_0, K, r, \sigma$) et de lancer les simulations sans toucher au code C++.
        \item Visualisation des résultats (graphiques de convergence, trajectoires) et intégration avec l'exécutable C++.
    \end{itemize}
\end{itemize}

Cette séparation claire (Back-end en C++/CUDA vs Front-end en Python) nous a permis d'avancer en parallèle : pendant que le moteur de calcul était optimisé, l'interface utilisateur était développée pour consommer les résultats produits.

\subsection{Outils et environnement collaboratif}

Le projet s’est appuyé sur les outils suivants :
\begin{itemize}
    \item \textbf{Git / GitHub} pour le suivi du développement et l’historique
    des itérations ;
    \item \textbf{CMake} pour la configuration et la compilation du projet ;
    \item \textbf{OpenMP} pour la parallélisation CPU ;
    \item \textbf{CUDA C++} pour l’implémentation GPU ;
    \item \textbf{Python} et \textbf{Streamlit} pour l’analyse et la visualisation
    des résultats ;
    \item \textbf{Overleaf} pour la rédaction du rapport.
\end{itemize}

L’utilisation intensive de Git a joué un rôle central, notamment pour gérer
les phases de refonte, corriger des erreurs structurelles (fichiers volumineux,
historique trop lourd) et stabiliser le dépôt en fin de projet.

\subsection{Gestion du temps et avancement}

Le travail a été réparti sur l’ensemble du semestre avec une montée en complexité
progressive :
\begin{itemize}
    \item début de semestre : compréhension théorique, premières implémentations
    CPU séquentielles ;
    \item milieu de semestre : restructuration du code, parallélisation OpenMP,
    premières analyses de performance ;
    \item fin de semestre : implémentation CUDA complète, optimisation mémoire,
    nettoyage du dépôt Git, analyse comparative et rédaction finale.
\end{itemize}

Cette organisation a permis d’anticiper les difficultés techniques, notamment
celles liées au calcul parallèle, à la gestion mémoire et aux limitations
structurelles de l’algorithme LSMC, tout en assurant la livraison d’un projet
fonctionnel, documenté et cohérent avec les objectifs pédagogiques du P1RV.



\section{Conclusion}

Ce projet a permis de concevoir, implémenter et analyser une version performante de l’algorithme de Longstaff–Schwartz pour le pricing d’options américaines, en combinant rigueur mathématique, validation numérique et étude approfondie des performances sur différentes architectures de calcul.

L’implémentation séquentielle sur CPU a servi de référence fonctionnelle et a permis de valider la cohérence de l’algorithme. La parallélisation via OpenMP a montré des gains mesurés mais limités, principalement contraints par la bande passante mémoire et la présence de phases séquentielles incompressibles. En revanche, l’implémentation GPU avec CUDA a démontré des accélérations significatives pour les phases massivement parallèles, en particulier la simulation Monte Carlo des trajectoires, avec des speedups pouvant atteindre un ordre de grandeur par rapport au CPU.

Les résultats obtenus mettent toutefois en évidence une limite structurelle fondamentale du LSMC : la backward induction impose une dépendance temporelle forte qui empêche toute parallélisation complète dans la dimension du temps. Le goulot d’étranglement réside donc dans l’estimation séquentielle de la valeur de continuation à chaque pas de temps, ce qui plafonne l’accélération théorique, même sur des architectures massivement parallèles.

La validation croisée avec des méthodes déterministes de différences finies a confirmé la cohérence numérique des prix obtenus, les écarts restant compatibles avec la variance statistique attendue d’une méthode de Monte Carlo. L’étude du choix des bases de régression a par ailleurs mis en évidence le compromis biais–variance inhérent à l’algorithme, ainsi que les limites pratiques des régressions polynomiales de degré élevé en présence de bruit stochastique.

En conclusion, ce travail montre que le GPU constitue une solution particulièrement adaptée pour les méthodes de Monte Carlo à grande échelle, tout en soulignant que les gains de performance sont intrinsèquement bornés par la structure algorithmique du LSMC. Ces résultats ouvrent la voie à des approches hybrides (batching d'options, parallélisme asynchrone) visant à contourner la barrière séquentielle temporelle, seule restriction à l'essor du LSMC sur les architectures exascale.


\clearpage
\appendix

\section{Résolution de l'EDS du GBM}
\label{appendix:eds}

On considère le processus \((S_t)_{t \ge 0}\) solution, sous la mesure risque-neutre \(\mathbb{Q}\), de l’équation différentielle stochastique
\[
\mathrm{d}S_t = r S_t\,\mathrm{d}t + \sigma S_t\,\mathrm{d}W_t^{\mathbb{Q}},
\]

où \(r\) est le taux sans risque, \(\sigma>0\) la volatilité constante, et \((W_t^{\mathbb{Q}})_{t\ge 0}\) un mouvement brownien standard sous \(\mathbb{Q}\).

Comme \(S_t>0\) presque sûrement, la fonction \(\ln\) est bien définie. En appliquant la formule d’Itô à \(f(S_t)=\ln(S_t)\), on obtient
\[
\mathrm{d}\ln(S_t)
=
f'(S_t)\,\mathrm{d}S_t
+\frac12 f''(S_t)\,(\mathrm{d}S_t)^2
=
\frac{1}{S_t}\,\mathrm{d}S_t
-\frac{1}{2S_t^2}(\mathrm{d}S_t)^2.
\]

En utilisant les règles du calcul stochastique
\[
(\mathrm{d}W_t^{\mathbb{Q}})^2=\mathrm{d}t,
\qquad
\mathrm{d}t\,\mathrm{d}W_t^{\mathbb{Q}}=0,
\qquad
(\mathrm{d}t)^2=0,
\]
on a
\[
\mathrm{d}S_t = r S_t\,\mathrm{d}t + \sigma S_t\,\mathrm{d}W_t^{\mathbb{Q}}
\quad \Longrightarrow \quad
(\mathrm{d}S_t)^2 = \sigma^2 S_t^2\,(\mathrm{d}W_t^{\mathbb{Q}})^2=\sigma^2 S_t^2\,\mathrm{d}t.
\]

En substituant dans la formule d’Itô, il vient
\[
\mathrm{d}\ln(S_t)
=
\frac{1}{S_t}\Bigl(r S_t\,\mathrm{d}t + \sigma S_t\,\mathrm{d}W_t^{\mathbb{Q}}\Bigr)
-\frac{1}{2S_t^2}\Bigl(\sigma^2 S_t^2\,\mathrm{d}t\Bigr),
\]
soit
\[
\mathrm{d}\ln(S_t)
=
\left(r-\frac{\sigma^2}{2}\right)\mathrm{d}t
+\sigma\,\mathrm{d}W_t^{\mathbb{Q}}.
\]

En intégrant entre \(t_n\) et \(t_{n+1}\) (avec \(\Delta t = t_{n+1}-t_n\)) :
\[
\ln(S_{t_{n+1}})-\ln(S_{t_n})
=
\left(r-\frac{\sigma^2}{2}\right)\Delta t
+\sigma\bigl(W_{t_{n+1}}^{\mathbb{Q}}-W_{t_n}^{\mathbb{Q}}\bigr).
\]

Les incréments du mouvement brownien sous \(\mathbb{Q}\) vérifient
\[
W_{t_{n+1}}^{\mathbb{Q}}-W_{t_n}^{\mathbb{Q}} \sim \mathcal{N}(0,\Delta t).
\]
Il existe donc \(Z_n \sim \mathcal{N}(0,1)\) tel que
\[
W_{t_{n+1}}^{\mathbb{Q}}-W_{t_n}^{\mathbb{Q}}=\sqrt{\Delta t}\,Z_n.
\]

En prenant l'exponentielle, on obtient la formule discrète exacte
\[
S_{t_{n+1}}
=
S_{t_n}\exp\!\left(
\left(r-\frac{\sigma^2}{2}\right)\Delta t
+\sigma\sqrt{\Delta t}\,Z_n
\right),
\]
et, en particulier, l’expression explicite en temps continu
\[
S_t
=
S_0\exp\!\left(
\left(r-\frac{\sigma^2}{2}\right)t
+\sigma W_t^{\mathbb{Q}}
\right).
\]




\section{Rappels de Probabilités et propriétés du Mouvement Brownien}
\label{appendix:brownian}

\subsection{Définition et propriétés du Mouvement Brownien}
Un mouvement brownien standard $(W_t)_{t \ge 0}$ est un processus stochastique caractérisé par :
\begin{enumerate}
    \item $W_0 = 0$ presque sûrement.
    \item Ses trajectoires $t \mapsto W_t$ sont continues.
    \item Ses accroissements sont indépendants et stationnaires (loi normale $\mathcal{N}(0, t-s)$ pour $W_t - W_s$).
\end{enumerate}

\subsection{Propriétés des incréments}
Pour la simulation numérique, nous utilisons la propriété d'indépendance des accroissements sur des intervalles disjoints. Si $0 \le t_1 < t_2 < \dots < t_n$, les variables aléatoires $(W_{t_{i+1}} - W_{t_i})$ sont mutuellement indépendantes.

\subsection{Loi des Grands Nombres et Monte Carlo}
La convergence des méthodes de Monte Carlo repose sur la Loi Forte des Grands Nombres (LFGN) :
\[ \lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^N f(X_i) = \mathbb{E}[f(X)] \quad \text{p.s.} \]
L'erreur d'approximation décroît en $\mathcal{O}(1/\sqrt{N})$, conformément au Théorème Central Limite.

\clearpage
\section{Notations et Conventions}
\label{appendix:notations}

\subsection{Probabilités et Processus Stochastiques}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}{|c|p{12cm}|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
$\Omega, \mathcal{F}, \mathbb{P}$ & Espace probabilisé filtré standard. \\
$\mathbb{Q}$ & Mesure risque-neutre, sous laquelle les actifs actualisés sont des martingales. \\
$(\mathcal{F}_t)_{t \ge 0}$ & Filtration naturelle engendrée par le mouvement brownien, représentant l'information disponible à l'instant $t$. \\
$W_t^{\mathbb{Q}}$ & Mouvement Brownien standard sous la mesure $\mathbb{Q}$. \\
$S_t$ & Valeur du sous-jacent à l'instant $t$. \\
$\mathbb{E}^{\mathbb{Q}}[\cdot | \mathcal{F}_t]$ & Espérance conditionnelle sous la mesure $\mathbb{Q}$ sachant l'information à l'instant $t$. \\
\hline
\end{longtable}
\end{center}

\subsection{Modélisation Financière}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}{|c|p{12cm}|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
$S_0$ & Prix initial du sous-jacent ($t=0$). \\
$K$ & Prix d'exercice (Strike) de l'option. \\
$T$ & Maturité de l'option (en années). \\
$r$ & Taux d'intérêt sans risque constant. \\
$\sigma$ & Volatilité constante du sous-jacent. \\
$\Phi(S)$ & Fonction de payoff ou valeur intrinsèque. Pour un Put : $\Phi(S) = (K-S)^+$. \\
$V_t$ & Prix de l'option américaine à l'instant $t$. \\
$C(t, S_t)$ & Valeur de continuation à l'instant $t$ sachant $S_t$. \\
$\tau$ & Temps d'arrêt correspondant à la stratégie d'exercice optimale. \\
\hline
\end{longtable}
\end{center}

\subsection{Méthodes Numériques (LSMC)}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}{|c|p{12cm}|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
$N \text{ ou } N_{steps}$ & Nombre de pas de temps de la discrétisation. \\
$\Delta t$ & Pas de temps uniforme ($\Delta t = T/N$). \\
$t_n$ & Dates de discrétisation ($t_n = n \Delta t$). \\
$M \text{ ou } N_{paths}$ & Nombre de trajectoires Monte Carlo simulées. \\
$S_{t_n}^{(i)}$ & Valeur simulée du sous-jacent pour la trajectoire $i$ à l'instant $t_n$. \\
$\psi_k(S)$ & $k$-ième fonction de base utilisée pour la régression. \\
$K_{reg}$ & Nombre total de fonctions de base (e.g., $3$ pour quadratique). \\
$\beta$ (ou $a_k$) & Vecteur des coefficients de régression estimés. \\
$\widehat{C}(t_n, S)$ & Estimateur numérique de la valeur de continuation obtenue par régression. \\
$A$ & Matrice de design (Vandermonde généralisée) utilisée dans les moindres carrés ($A^T A \beta = A^T Y$). \\
\hline
\end{longtable}
\end{center}

\subsection{Implémentation et Architecture}
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{longtable}{|c|p{12cm}|}
\hline
\textbf{Notation} & \textbf{Description} \\
\hline
OpenMP & \textit{Open Multi-Processing}, API pour la programmation parallèle sur mémoire partagée (CPU). \\
CUDA & \textit{Compute Unified Device Architecture}, plateforme de calcul parallèle NVIDIA. \\
Thread & Unité d'exécution de base. Sur GPU, un thread traite typiquement une trajectoire simulée. \\
Block & Regroupement de threads sur GPU partageant une mémoire locale (Shared Memory). \\
Grid & Ensemble des blocs lancés pour l'exécution d'un Kernel CUDA. \\
Kernel & Fonction exécutée sur le GPU par chaque thread de la grille. \\
Speedup & Gain de performance, défini comme $\text{Temps}_{CPU} / \text{Temps}_{GPU}$. \\
\hline
\end{longtable}
\end{center}

\clearpage
\begin{thebibliography}{9}

\bibitem{LSM2001}
Longstaff, F. A., \& Schwartz, E. S. (2001).
\textit{Valuing American Options by Simulation: A Simple Least-Squares Approach}.
The Review of Financial Studies, 14(1), 113-147.

\bibitem{Hull}
Hull, J. C.
\textit{Options, Futures, and Other Derivatives}.
Pearson Education.

\bibitem{Glasserman}
Glasserman, P. (2004).
\textit{Monte Carlo Methods in Financial Engineering}.
Springer.

\bibitem{Oger}
Oger, G.
\textit{Mémoire de Magistère : Valorisation d'options américaines sur GPU}.

\bibitem{Croain}
Croain, D., \& Poulette.
\textit{Projet GPU : Pricing d'options américaines}.

\bibitem{Risks}
Risks Journal (2023).
\textit{Recent Advances in American Option Pricing}.
risks-11-00145.


\bibitem{Benguigui}
Benguigui, M. (2015).
\textit{Valorisation d'options américaines et Value At Risk sur cluster GPU/CPU hétérogène}.
Thèse de doctorat, Université Nice Sophia Antipolis.

\bibitem{Reesor}
Reesor, R. M., Stentoft, L., \& Zhu, X. (2024).
\textit{A Critical Analysis of the Weighted Least Squares Monte Carlo Method for Pricing American Options}.
Finance Research Letters.

\bibitem{ArealParallelMethods}
Areal, N., Rodrigues, A., \& Armada, M. J. R.
\textit{Improvements to the Least Squares Monte Carlo Option Valuation Method}.
Source file: \texttt{Areal\_Parallel\_Methods.pdf}.

\bibitem{Detra}
Detra (2023).
\textit{Risk Management with Local Least Squares Monte Carlo}.
Technical Note.
\end{thebibliography}

\end{document}
